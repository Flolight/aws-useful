{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be16fb0a-2815-40b1-8a92-2e1c80076f66",
   "metadata": {},
   "source": [
    "# How to serve the `stabilityai/stable-cascade` model on Amazon SageMaker\n",
    "\n",
    "Stable Cascade project links:\n",
    "* [Project page](https://www.timothybrooks.com/instruct-pix2pix/)\n",
    "* [GitHub repositroy](https://github.com/Stability-AI/StableCascade)\n",
    "* [HuggingFace model hub page](https://huggingface.co/stabilityai/stable-cascade) (model ID: `stabilityai/stable-cascade`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676873b3-9a5a-4b4f-beef-89540d81023f",
   "metadata": {},
   "source": [
    "## 1. Dependency installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e1dce2-53ee-4de0-9fae-c6d3883712cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pip --upgrade --quiet\n",
    "%pip install huggingface_hub==0.20.3 sagemaker==2.199.0 diffusers==0.27.2 sagemaker-ssh-helper --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd7c3d-d5fa-42da-9f03-8159d70a57cf",
   "metadata": {},
   "source": [
    "## 2. Imports & variable assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518633b5-2da4-4861-9223-7299e35b0d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import shutil\n",
    "import tarfile\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "from diffusers.utils import make_image_grid\n",
    "import huggingface_hub\n",
    "import sagemaker\n",
    "import sagemaker.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e4865f-9600-4eee-bc4d-8c610de2af4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"0\" # Put it to 1 if do not want the progress of the downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1b3f85-b291-4db4-80a3-325c0e7f6f26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "SM_DEFAULT_EXECUTION_ROLE_ARN = sagemaker.get_execution_role()\n",
    "SM_SESSION = sagemaker.session.Session()\n",
    "SM_ARTIFACT_BUCKET_NAME = SM_SESSION.default_bucket() # We use SageMaker's default bucket\n",
    "\n",
    "REGION_NAME = SM_SESSION._region_name\n",
    "S3_CLIENT = boto3.client(\"s3\", region_name=REGION_NAME)\n",
    "SAGEMAKER_CLIENT = boto3.client(\"sagemaker\", region_name=REGION_NAME)\n",
    "SAGEMAKER_RUNTIME_CLIENT = boto3.client(\"sagemaker-runtime\", region_name=REGION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707d748d-1b01-49f1-8669-a8dd1bd09010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ[\"HOME\"]\n",
    "\n",
    "# HuggingFace local model storage\n",
    "HF_LOCAL_CACHE_DIR = Path(HOME_DIR) / \".cache\" / \"huggingface\" / \"hub\"\n",
    "HF_LOCAL_DOWNLOAD_DIR = Path.cwd() / \"model_repo\"\n",
    "HF_LOCAL_PRIOR_DOWNLOAD_DIR = Path.cwd() / \"prior_model_repo\"\n",
    "HF_LOCAL_DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "HF_LOCAL_PRIOR_DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Selected HuggingFace model\n",
    "HF_HUB_PRIOR_MODEL_NAME = \"stabilityai/stable-cascade-prior\"\n",
    "HF_HUB_MODEL_NAME = \"stabilityai/stable-cascade\"\n",
    "\n",
    "# HuggingFace remote model storage (Amazon S3)\n",
    "MODEL_ARTIFACTS_KEY_PREFIX = f\"photobooth/endpoint/{HF_HUB_MODEL_NAME}/model\"\n",
    "CODE_ARTIFACTS_KEY_PREFIX = f\"photobooth/endpoint/{HF_HUB_MODEL_NAME}/code\"\n",
    "\n",
    "PRIOR_MODEL_ARTIFACTS_KEY_PREFIX = f\"photobooth/endpoint/{HF_HUB_PRIOR_MODEL_NAME}/model\"\n",
    "PRIOR_CODE_ARTIFACTS_KEY_PREFIX = f\"photobooth/endpoint/{HF_HUB_PRIOR_MODEL_NAME}/code\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb6256-890c-41ff-87f7-23f206dc5801",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Assets deployment: model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b363f06-3268-4ba4-a355-e48d60a7203e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea57b1b949574d5cbb51fe900d30707f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 36 files:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b267e1ba39364c5eb5ff51d827e5ac71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "decoder/config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aace47edeba8405388a7a4399854dd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "super_resolution.safetensors:   0%|          | 0.00/814M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a689e6e01341e3adca2a69b10580bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "inpainting.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a12ed48ae244d2abb6a31365410ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/6.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df082c9da6b425eaae67a77dbf282bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "canny.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21149ce1e1f47ddaa5cd51e8c9f2715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stable_cascade_stage_b.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad4fc7decd5448691effbd9ed92b36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stable_cascade_stage_c.safetensors:   0%|          | 0.00/9.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e7f3c3d22b40d38b4a27ad3a47bb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.bf16.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1162d8469f3348c7b08e6b852ad74e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "decoder_lite/config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb44666bf2b4439da6d86f4722373bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.bf16.safetensors:   0%|          | 0.00/1.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d81ffdd0814d2d8cc3c0dbb85d5000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/2.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a25a07dbbc242e7945059766ea41126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "effnet_encoder.safetensors:   0%|          | 0.00/81.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2add13d2e6744f24acf887c011631942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c5f3dfcdf6483b896c94eb78d25016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "previewer.safetensors:   0%|          | 0.00/16.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a6477de6fe4a618dc74d306c68758a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6024a403b2f4479e856727af23a3b32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage_a.safetensors:   0%|          | 0.00/73.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107b16bf774b4d019c37261528299507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage_b.safetensors:   0%|          | 0.00/6.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd4c111574e4ca1a8b8d32b1885170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage_b_bf16.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311894ad7ca141e394d99bf771ea1990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage_b_lite.safetensors:   0%|          | 0.00/2.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=HF_HUB_MODEL_NAME,\n",
    "    revision=\"main\",\n",
    "    local_dir=HF_LOCAL_DOWNLOAD_DIR,\n",
    "    local_dir_use_symlinks=\"auto\",  # Files larger than 5MB are actually symlinked to the local HF cache\n",
    "    allow_patterns=[\"*.json\", \"*.txt\", \"*.safetensors\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1c408-4f21-4803-803d-efbb2d98378b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_artifacts_url = SM_SESSION.upload_data(\n",
    "    path=HF_LOCAL_DOWNLOAD_DIR.as_posix(),\n",
    "    bucket=SM_ARTIFACT_BUCKET_NAME,\n",
    "    key_prefix=MODEL_ARTIFACTS_KEY_PREFIX,\n",
    ")\n",
    "print(f\"Model artifacts have been successfully uploaded to: {model_artifacts_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff546579-d400-47e3-aa15-4707971fad56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=HF_HUB_PRIOR_MODEL_NAME,\n",
    "    revision=\"main\",\n",
    "    local_dir=HF_LOCAL_PRIOR_DOWNLOAD_DIR,\n",
    "    local_dir_use_symlinks=\"auto\",  # Files larger than 5MB are actually symlinked to the local HF cache\n",
    "    allow_patterns=[\"*.json\", \"*.txt\", \"*.safetensors\"], # Falcon is currently implemented as a custom model, we must include the .py files to be able to use it\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770a263-4d5b-4271-af78-abf0f8bfb3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_model_artifacts_url = SM_SESSION.upload_data(\n",
    "    path=HF_LOCAL_PRIOR_DOWNLOAD_DIR.as_posix(),\n",
    "    bucket=SM_ARTIFACT_BUCKET_NAME,\n",
    "    key_prefix=PRIOR_MODEL_ARTIFACTS_KEY_PREFIX,\n",
    ")\n",
    "print(f\"Prior Model artifacts have been successfully uploaded to: {model_artifacts_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacf6a0-7557-41c3-8192-79dae4ce9857",
   "metadata": {},
   "source": [
    "## 4. Assets deployment: code artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d33a1e5-039a-4a3b-aa2c-ead07c21b59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code artifacts local storage\n",
    "SOURCE_DIR = Path.cwd() / \"code\"\n",
    "SOURCE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a28b890c-b3ce-4b13-8350-4eb0db47f851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/GenAI-PhotoBooth/Cascade/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SOURCE_DIR}/requirements.txt\n",
    "transformers==4.39.1\n",
    "#diffusers==0.26.1\n",
    "#diffusers @ git+https://github.com/kashif/diffusers.git@wuerstchen-v3\n",
    "#diffusers @ git+https://github.com/kashif/diffusers.git@a3dc21385b7386beb3dab3a9845962ede6765887\n",
    "diffusers==0.27.2\n",
    "accelerate==0.28.0\n",
    "peft==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df172b99-6d51-4a7c-a8b3-9b227f5b74ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableCascadeDecoderPipeline, StableCascadePriorPipeline\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline \u001b[38;5;241m=\u001b[39m StableCascadeDecoderPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n",
    "import torch\n",
    "\n",
    "self._pipeline = StableCascadeDecoderPipeline.from_pretrained(\"./\", torch_dtype=torch.float16).to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7be795e2-aa2d-46f8-aacc-2957b28a14f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/GenAI-PhotoBooth/Cascade/code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SOURCE_DIR}/serving.properties\n",
    "engine=Python\n",
    "option.model_id=s3://sagemaker-us-east-1-433808754371/photobooth/endpoint/stabilityai/stable-cascade/model\n",
    "option.dtype=bf16\n",
    "option.entryPoint=handler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1e19b7a-6297-4f64-b742-6f2b83c74838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/GenAI-PhotoBooth/Cascade/code/handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SOURCE_DIR}/handler.py\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import PIL\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n",
    "\n",
    "from djl_python import Input, Output\n",
    "import torch\n",
    "\n",
    "def encode_image(image: PIL.Image, format: str) -> str:\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format=format)\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\") # could be .decode(\"ascii\") too\n",
    "\n",
    "def decode_image(encoded_image: str) -> PIL.Image:\n",
    "    image_data_bytes = base64.b64decode(encoded_image)\n",
    "    return PIL.Image.open(io.BytesIO(image_data_bytes))\n",
    "\n",
    "\n",
    "def get_torch_dtype_from_str(dtype: str) -> torch.dtype:\n",
    "    if dtype == \"fp32\":\n",
    "        return torch.float32\n",
    "    if dtype == \"fp16\":\n",
    "        return torch.float16\n",
    "    if dtype == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if dtype == \"int8\":\n",
    "        return torch.int8\n",
    "    if dtype is None:\n",
    "        return None\n",
    "    raise ValueError(f\"Data type cannot be parsed as valid Torch data type: {dtype}\")\n",
    "    \n",
    "    \n",
    "class InferenceService:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._device = \"cuda\"\n",
    "        self.initialized = False\n",
    "        self.num_images_per_prompt = 1\n",
    "        \n",
    "    def initialize(self, properties: Dict[str, str]) -> None:\n",
    "        # The `option.model_id` variable can be either a HuggingFace ID, an S3 URL, or a local directory path.\n",
    "        # - If `option.model_id` is a HuggingFace ID, models artifacts are downloaded by the \n",
    "        #  Transformers/Diffusers library (slower).\n",
    "        # - If `option.model_id` is an S3 URL, the DJL model server downloads the model artifacts locally using \n",
    "        #   the highly efficient `s5cmd` tool and set `option.model_id` to the local download directory path.\n",
    "        # - If `option.model_id` is not available, it is assumed tat model artifacts are located in \n",
    "        #   `option.model_dir`. If `option.model_dir` is not available, `/opt/ml/model` is used as default value.\n",
    "        \n",
    "        model_location = properties.get(\"model_id\") or properties.get(\"model_dir\")\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        print(f\"model_location: {model_location}\")\n",
    "        #torch_dtype = get_torch_dtype_from_str(dtype=properties.get(\"dtype\", \"fp16\"))\n",
    "        \n",
    "        #print(f\"model_dir content: {os.listdir(model_dir)}\")\n",
    "        print(f\"model_location content: {os.listdir(model_location)}\")\n",
    "        prior_location = f\"{model_location}/prior\"\n",
    "        print(f\"prior content: {os.listdir(prior_location)}\")\n",
    "        \n",
    "        self._pipeline = StableCascadeDecoderPipeline.from_pretrained(model_location, torch_dtype=torch.float16).to(self._device)\n",
    "        print(\"main loaded\")\n",
    "        print(\"start loading prior\")\n",
    "        self._prior_pipeline = StableCascadePriorPipeline.from_pretrained(f\"{model_location}/prior\", variant=\"bf16\", torch_dtype=torch.bfloat16).to(self._device)\n",
    "        print(\"prior loaded\")\n",
    "        \n",
    "        #self._pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_location, torch_dtype=torch_dtype, safety_checker=None)\n",
    "        self._pipeline.to(self._device)\n",
    "        #self._prior_pipeline.to(self._device)\n",
    "        self._pipeline.enable_model_cpu_offload()\n",
    "        self.initialized = True\n",
    "        print(\"end initialize\")\n",
    "    \n",
    "    def handle_request(self, inputs: Input) -> Output:\n",
    "        request_payload = inputs.get_as_json()\n",
    "        prompt, negative_prompt = request_payload[\"prompt\"], request_payload[\"negative_prompt\"]\n",
    "        image = decode_image(encoded_image=request_payload[\"image\"])\n",
    "        guidance_scale = request_payload[\"guidance_scale\"]\n",
    "        generation_parameters = request_payload[\"generation_parameters\"]\n",
    "        num_inference_steps = request_payload[\"num_inference_steps\"]\n",
    "        seed = generation_parameters.pop(\"seed\", None)\n",
    "        \n",
    "        print(f\"prompt: {prompt}\")\n",
    "        print(f\"negative_prompt: {negative_prompt}\")\n",
    "        print(f\"image: {image}\")\n",
    "        print(f\"guidance_scale: {guidance_scale}\")\n",
    "        print(f\"generation_parameters: {generation_parameters}\")\n",
    "        print(f\"num_inference_steps: {num_inference_steps}\")\n",
    "        print(f\"seed: {seed}\")\n",
    "        print(f\"self.num_images_per_prompt: {self.num_images_per_prompt}\")\n",
    "\n",
    "        \n",
    "        if seed:\n",
    "            generation_parameters[\"generator\"] = torch.Generator(device=self._device).manual_seed(int(seed))\n",
    "        prior_output  = self._prior_pipeline(\n",
    "            images=image,\n",
    "            prompt=prompt,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            negative_prompt=negative_prompt,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_images_per_prompt=self.num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps\n",
    "        )\n",
    "        print(f\"Prior output: {prior_output}\")\n",
    "        \n",
    "        decoder_output = self._pipeline(\n",
    "            image_embeddings=prior_output.image_embeddings.half(),\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps\n",
    "        )\n",
    "        serialized_output_image = encode_image(image=decoder_output.images[0], format=\"PNG\")\n",
    "        return Output().add(serialized_output_image).add_property(\"content-type\", \"image/png\")\n",
    "\n",
    "    \n",
    "_service = InferenceService()\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> Optional[Output]:\n",
    "    if not _service.initialized:\n",
    "        print(\"Initializing inference service\")\n",
    "        _service.initialize(properties=inputs.get_properties())\n",
    "        \n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "    return _service.handle_request(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1577204-0c56-401a-9299-58194c44c62f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code artifacts have been successfully uploaded to: s3://sagemaker-us-east-1-433808754371/photobooth/endpoint/stabilityai/stable-cascade/code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "archive_file_path = Path(\"model.tar.gz\")\n",
    "with tarfile.open(archive_file_path, mode=\"w:gz\") as tar:\n",
    "    tar.add(SOURCE_DIR, arcname=\".\")\n",
    "    \n",
    "code_artifacts_uri = SM_SESSION.upload_data(\n",
    "    path=archive_file_path.as_posix(),\n",
    "    bucket=SM_ARTIFACT_BUCKET_NAME,\n",
    "    key_prefix=CODE_ARTIFACTS_KEY_PREFIX,\n",
    ")\n",
    "\n",
    "print(f\"Code artifacts have been successfully uploaded to: {code_artifacts_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15e16c-338d-4647-af7d-0973288e63f5",
   "metadata": {},
   "source": [
    "## 5. Endpoint deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2db9c44-dbc5-42e7-9375-cfbd0803e8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your Amazon SageMaker service quota for \"<instance_type> for endpoint usage\"\n",
    "endpoint_instance_type = \"ml.g5.2xlarge\"\n",
    "# We use the SageMaker Large Model Inference (LMI) Deep Learning Containers (DLC) image\n",
    "container_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", \n",
    "    region=REGION_NAME, \n",
    "    version=\"0.25.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be1e8f6d-8fc3-45e3-b8b5-d94be2258e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Model ARN: arn:aws:sagemaker:us-east-1:433808754371:model/photobooth-stable-cascade-model-2024-03-25-23-32-30-553\n",
      "Created EndpointConfig ARN: arn:aws:sagemaker:us-east-1:433808754371:endpoint-config/photobooth-stable-cascade-endpoint-config-2024-03-25-23-32-30-5\n"
     ]
    }
   ],
   "source": [
    "from sagemaker_ssh_helper.wrapper import SSHModelWrapper\n",
    "\n",
    "timestamp = sagemaker.utils.sagemaker_timestamp()\n",
    "\n",
    "model_name = f\"photobooth-stable-cascade-model-{timestamp}\"[:63]\n",
    "\n",
    "create_model_response = SAGEMAKER_CLIENT.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=SM_DEFAULT_EXECUTION_ROLE_ARN,\n",
    "    PrimaryContainer={\"Image\": container_image_uri, \"ModelDataUrl\": code_artifacts_uri},\n",
    ")\n",
    "\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "print(f\"Created Model ARN: {model_arn}\")\n",
    "\n",
    "endpoint_config_name = f\"photobooth-stable-cascade-endpoint-config-{timestamp}\"[:63]\n",
    "endpoint_config_response = SAGEMAKER_CLIENT.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 1.0,\n",
    "            \"InstanceType\": endpoint_instance_type,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15 * 60,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 15 * 60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "endpoint_config_arn = endpoint_config_response[\"EndpointConfigArn\"]\n",
    "print(f\"Created EndpointConfig ARN: {endpoint_config_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9b10629-651a-4e90-81bf-9c9da778bfc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint ARN: arn:aws:sagemaker:us-east-1:433808754371:endpoint/photobooth-stable-cascade-endpoint-2024-03-25-23-32-30-553\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Failed\n",
      "Final status: Failed\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = f\"photobooth-stable-cascade-endpoint-{timestamp}\"[:63]\n",
    "\n",
    "create_endpoint_response = SAGEMAKER_CLIENT.create_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "endpoint_arn = create_endpoint_response['EndpointArn']\n",
    "print(f\"Endpoint ARN: {endpoint_arn}\")\n",
    "\n",
    "describe_endpoint_response = SAGEMAKER_CLIENT.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = describe_endpoint_response[\"EndpointStatus\"]\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(30)\n",
    "    describe_endpoint_response = SAGEMAKER_CLIENT.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = describe_endpoint_response[\"EndpointStatus\"]\n",
    "    print(f\"Status: {status}\")\n",
    "print(f\"Final status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95181113-1b8f-4040-8b2e-97cb6dd4f2ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Endpoint invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c304938c-8315-41a9-947d-2a1a088ab30e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Image data bytes are serialized into a base64-encoded string of text. Base64 encoding encode 3 bytes of binary data into 4 ASCII characters. Each ASCII character is represented using 1 byte so the encoded data is 33% larger than the raw data. However, encoded data is a string of text that can be serialized into JSON for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e904911b-2d01-40d0-ada3-44631afac899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_file_image(file_path: Path) -> str:\n",
    "    with open(file_path, mode=\"rb\") as file_handle:\n",
    "        image_data_bytes = file_handle.read()\n",
    "        return base64.b64encode(image_data_bytes).decode(\"utf-8\") # could be .decode(\"ascii\") too\n",
    "\n",
    "    \n",
    "def encode_image(image: PIL.Image, format: str) -> str:\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format=format)\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\") # could be .decode(\"ascii\") too\n",
    "\n",
    "\n",
    "def decode_image(encoded_image: str) -> PIL.Image:\n",
    "    image_data_bytes = base64.b64decode(encoded_image)\n",
    "    return PIL.Image.open(io.BytesIO(image_data_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b78e21a6-bf0e-4dd7-bec5-e5c71cbd6c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prompt = \"Anthropomorphic cat dressed as a pilot\"\n",
    "prompt = \"portrait of a man in lineart style\"# 1900's wering sun glasses\"\n",
    "negative_prompt = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0d0f264b-7bf6-451a-8eda-708eb01b7f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "#image_file_path = Path(\"portrait.png\")\n",
    "image_file_path = Path(\"portrait.jpg\")\n",
    "input_image = PIL.Image.open(image_file_path)\n",
    "input_image.size\n",
    "images = [input_image]\n",
    "for im in images:\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b4bb9c99-cab4-4e7f-aefb-87a6e912b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"make it line art style\",\n",
    "generation_parameters = {\n",
    "    \"seed\": 42,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"image_guidance_scale\":  1.5, \n",
    "    \"guidance_scale\": 9.0,\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"image\": encode_image(image=input_image, format=\"JPEG\"),\n",
    "    \"prompt\": prompt,\n",
    "    \"generation_parameters\": generation_parameters,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "51ed0ec3-5aeb-4476-8591-f3d5f6998f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_parameters = {\n",
    "    #\"prompt\": prompt,\n",
    "    #\"height\": 1024,\n",
    "    #\"width\": 1024,\n",
    "    #\"negative_prompt\": negative_prompt,\n",
    "    #\"guidance_scale\": 2.0,\n",
    "    \"num_images_per_prompt\": 1,\n",
    "    #\"num_inference_steps\": 40,\n",
    "    #\"seed\": 456809\n",
    "}\n",
    "payload = {\n",
    "    \"image\": encode_image(image=input_image, format=\"JPEG\"),\n",
    "    #\"image\":\"\",\n",
    "    \"prompt\": prompt,\n",
    "    \"negative_prompt\": negative_prompt,\n",
    "    \"generation_parameters\": generation_parameters,\n",
    "    \"guidance_scale\": 1, # should be < 1\n",
    "    \"num_inference_steps\": 20,\n",
    "    #\"num_images_per_prompt\": 1\n",
    "}\n",
    "#endpoint_name= \"photobooth-stable-cascade-endpoint-2024-02-27-11-03-02-293\"\n",
    "#json.dumps(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "49232fa7-f357-42fe-81b7-b2531e847a21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\n  \"code\":424,\n  \"message\":\"prediction failure\",\n  \"error\":\"Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor\"\n}\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/photobooth-stable-cascade-endpoint-2024-02-27-14-54-29-162 in account 433808754371 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Warning: SageMaker endpoints have a fixed 60s timeout per invocation (non-configurable)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m invoke_endpoint_response \u001b[38;5;241m=\u001b[39m \u001b[43mSAGEMAKER_RUNTIME_CLIENT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#invoke_endpoint_response\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\n  \"code\":424,\n  \"message\":\"prediction failure\",\n  \"error\":\"Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor\"\n}\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/photobooth-stable-cascade-endpoint-2024-02-27-14-54-29-162 in account 433808754371 for more information."
     ]
    }
   ],
   "source": [
    "# Warning: SageMaker endpoints have a fixed 60s timeout per invocation (non-configurable)\n",
    "invoke_endpoint_response = SAGEMAKER_RUNTIME_CLIENT.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "#invoke_endpoint_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad677e2-4120-4daa-b6ab-d9f06a76143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = invoke_endpoint_response['Body'].read()\n",
    "output_image = decode_image(encoded_image=response_body)\n",
    "make_image_grid([input_image, output_image], rows=1, cols=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7661fd93-08ed-4f11-a9a6-debe0a1ebf4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a8913-21b4-4d42-bf0e-ddbc436c1eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d6a71a1-3afb-4673-8cdf-4affb64cc4ae",
   "metadata": {},
   "source": [
    "## 7. Clean-up\n",
    "#### Delete AWS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3a5a9-e405-4e9c-852e-84f06fe31b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint_response = SAGEMAKER_CLIENT.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Deleted Endpoint: {endpoint_name}\")\n",
    "delete_endpoint_config_response = SAGEMAKER_CLIENT.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "print(f\"Deleted EndpointConfig: {endpoint_config_name}\")\n",
    "delete_model_response = SAGEMAKER_CLIENT.delete_model(ModelName=model_name)\n",
    "print(f\"Deleted Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bab16-bbe9-4665-8bbc-7a156949fcdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Delete remote assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb61d92-37d0-41fd-b1b7-e98e643f8e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_s3_objects_by_prefix(bucket_name: str, key_prefix: str) -> None:\n",
    "    paginator = S3_CLIENT.get_paginator(\"list_objects\")\n",
    "    operation_parameters = {\"Bucket\": bucket_name, \"Prefix\": key_prefix}\n",
    "    page_iterator = paginator.paginate(**operation_parameters)\n",
    "    keys = [obj[\"Key\"] for page in page_iterator for obj in page[\"Contents\"]]\n",
    "    S3_CLIENT.delete_objects(Bucket=bucket_name, Delete={\"Objects\": [{\"Key\": key} for key in keys]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bd4a6-a2b8-40d2-a521-c729754bc3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove code & model artifacts from S3\n",
    "delete_s3_objects_by_prefix(bucket_name=SM_ARTIFACT_BUCKET_NAME, key_prefix=MODEL_ARTIFACTS_KEY_PREFIX)\n",
    "delete_s3_objects_by_prefix(bucket_name=SM_ARTIFACT_BUCKET_NAME, key_prefix=CODE_ARTIFACTS_KEY_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6fe23-7712-4ae6-a3cb-480b40f62013",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Delete local assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ca53a-9269-4130-a473-15c8307b17de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_local_model_cache_dir(hf_model_name: str) -> str:\n",
    "    for dir_name in os.listdir(HF_LOCAL_CACHE_DIR):\n",
    "        if dir_name.endswith(hf_model_name.replace(\"/\", \"--\")):\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find HF local cache directory for model {hf_model_name}\")\n",
    "    return HF_LOCAL_CACHE_DIR / dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d249c-251f-4bad-a2ba-66e29d0ac56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove model artifacts from the local download directory\n",
    "shutil.rmtree(HF_LOCAL_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cdcbdf-5173-423d-a035-8a4d337521af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove model artifacts from the local HuggingFace cache directory\n",
    "hf_local_cache_dir = get_local_model_cache_dir(hf_model_name=HF_HUB_MODEL_NAME)\n",
    "shutil.rmtree(hf_local_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959e2b3-d522-4619-bdf7-bdc0b076c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove code artifacts from the local host\n",
    "shutil.rmtree(SOURCE_DIR)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
